{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fba915a1-8a65-4f93-b2f5-56acb7082295",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob, re, os\n",
    "import xarray as xr, xesmf as xe, numpy as np\n",
    "from tcpyPI.pi import pi\n",
    "\n",
    "from xclim.core.units import convert_units_to\n",
    "from datetime import datetime\n",
    "\n",
    "from  IPython.display import clear_output\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category = FutureWarning)\n",
    "warnings.filterwarnings(\"ignore\", message = \".+multiple fill values.+\")\n",
    "\n",
    "xn,xx,yn,yx = [100,145,0,40]\n",
    "rnm = \"philippines\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b85a2816-26e4-4662-bdb8-8ffed36127e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def wrap_lon(ds):\n",
    "    \n",
    "    # method to wrap longitude from (0,360) to (-180,180)\n",
    "    if \"longitude\" in ds.coords: ds = ds.rename(longitude = \"lon\", latitude = \"lat\")\n",
    "    \n",
    "    if ds.lon.max() > 180:\n",
    "        ds[\"lon\"] = (ds.lon.dims, (((ds.lon.values + 180) % 360) - 180), ds.lon.attrs)\n",
    "        \n",
    "    ds = ds.reindex({ \"lon\" : np.sort(ds.lon) })\n",
    "    ds = ds.reindex({ \"lat\" : np.sort(ds.lat) })\n",
    "    \n",
    "    return ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "779d2b73-ffde-4101-b1a4-2ee3c9a559e8",
   "metadata": {},
   "source": [
    "# Identify candidate models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d81699a3-bfbf-4aa0-94af-a94cca06d175",
   "metadata": {},
   "outputs": [],
   "source": [
    "# list all available file paths\n",
    "fplist = glob.glob(\"/badc/cmip6/data/CMIP6/ScenarioMIP/*/*/ssp585/r1i1p1f1/Omon/tos/*\")\n",
    "\n",
    "# get unique models\n",
    "mlist = list(set([\"/\".join(fp.split(\"/\")[6:10]) for fp in fplist]))\n",
    "\n",
    "# find duplicates\n",
    "mdup = [m for m in mlist if sum([m in fp for fp in fplist]) > 1]\n",
    "\n",
    "# list one instance for each model\n",
    "fp_unique = [[fp for fp in fplist if m in fp][0] for m in mlist if sum([m in fp for fp in fplist]) == 1]\n",
    "fp_dup = [[fp for fp in fplist if m in fp and \"gr\" in fp][0] for m in mdup]\n",
    "\n",
    "# for models where r1i1p1f1 doesn't exist, get first ensemble member\n",
    "fplist_notr1 = [fp for fp in glob.glob(\"/badc/cmip6/data/CMIP6/ScenarioMIP/*/*/ssp585/*/Omon/tos/*\") if not \"r1i1p1f1\" in fp]\n",
    "mlist_notr1 = list(set([\"/\".join(fp.split(\"/\")[6:8]) for fp in fplist_notr1]))\n",
    "mlist_notr1 = [m for m in mlist_notr1 if not m in list(set([\"/\".join(fp.split(\"/\")[6:8]) for fp in fplist]))]\n",
    "fp_notr1 = [sorted([fp for fp in fplist_notr1 if m in fp])[0] for m in mlist_notr1]\n",
    "\n",
    "# create final list of models from all of the above\n",
    "fplist = sorted(fp_unique + fp_dup + fp_notr1)\n",
    "\n",
    "excl = ['/badc/cmip6/data/CMIP6/ScenarioMIP/AWI/AWI-CM-1-1-MR/ssp585/r1i1p1f1/Omon/tos/gn',\n",
    "        '/badc/cmip6/data/CMIP6/ScenarioMIP/CNRM-CERFACS/CNRM-CM6-1-HR/ssp585/r1i1p1f2/Omon/tos/gn',\n",
    "        '/badc/cmip6/data/CMIP6/ScenarioMIP/FIO-QLNM/FIO-ESM-2-0/ssp585/r1i1p1f1/Omon/tos/gn',\n",
    "        '/badc/cmip6/data/CMIP6/ScenarioMIP/MOHC/HadGEM3-GC31-MM/ssp585/r1i1p1f3/Omon/tos/gn',\n",
    "        '/badc/cmip6/data/CMIP6/ScenarioMIP/NCAR/CESM2-WACCM/ssp585/r2i1p1f1/Omon/tos/gn']\n",
    "\n",
    "fplist = [fp for fp in fplist if not fp in excl]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d72c5f9c-5633-422b-89e4-77d2187dbb1d",
   "metadata": {},
   "source": [
    "# Compile MSLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1f5b17f1-b1e6-43f3-b133-c9fa5f0ff54e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n"
     ]
    }
   ],
   "source": [
    "for fp in fplist:\n",
    "\n",
    "    mdl = fp.split(\"/\")[7]+\"_\"+fp.split(\"/\")[9]\n",
    "    print(mdl)\n",
    "\n",
    "    psl_fp = \"/\".join(fp.split(\"/\")[:-3])+\"/Amon/psl/*/latest/*\"\n",
    "\n",
    "    fl_hist = glob.glob(re.sub(\"ScenarioMIP\", \"CMIP\", re.sub(\"ssp585\", \"historical\", psl_fp)))\n",
    "    fl_ssp = [fnm for fnm in glob.glob(psl_fp) if int(fnm[-16:-12]) <= 2100]\n",
    "\n",
    "    if len(fl_hist) == 0 or len(fl_ssp) == 0: continue\n",
    "    \n",
    "    new_fnm = \"/home/users/clairb/potential-intensity/psl/\"+rnm+\"_\"+re.sub(\"historical_\",\"\",fl_hist[0]).split(\"/\")[-1][:-9]+fl_ssp[-1].split(\"/\")[-1][-9:]\n",
    "    if os.path.exists(new_fnm): continue\n",
    "\n",
    "    ds_hist = [xr.open_dataset(fnm).psl.convert_calendar(\"standard\", align_on = \"date\") for fnm in fl_hist]\n",
    "    ds_ssp = [xr.open_dataset(fnm).psl.convert_calendar(\"standard\", align_on = \"date\") for fnm in fl_ssp]\n",
    "\n",
    "    ds = wrap_lon(xr.concat(ds_hist + ds_ssp, \"time\"))\n",
    "    ds = ds.sel(lon = slice(xn,xx), lat = slice(yn,yx))\n",
    "    ds = convert_units_to(ds, \"hPa\")\n",
    "\n",
    "    ds.to_netcdf(new_fnm)\n",
    "clear_output(wait = False)\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "146cc908-0de4-406d-b99c-a3764dbf3f5e",
   "metadata": {},
   "source": [
    "# Regrid & compile SSTs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c932bc55-28d3-4be9-b37f-a3b635257242",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n"
     ]
    }
   ],
   "source": [
    "for fp in fplist:\n",
    "    \n",
    "    print(fp, end = \": \")\n",
    "\n",
    "    fl_hist = glob.glob(re.sub(\"ScenarioMIP\", \"CMIP\", re.sub(\"ssp585\", \"historical\", fp)) + \"/latest/*\")\n",
    "    fl_ssp = [fnm for fnm in glob.glob(fp+\"/latest/*\") if int(fnm[-16:-12]) <= 2100]\n",
    "\n",
    "    if len(fl_hist) == 0 or len(fl_ssp) == 0: \n",
    "        print(\"missing data\")\n",
    "        continue\n",
    "    \n",
    "    new_fnm = \"/home/users/clairb/potential-intensity/tos/\"+rnm+\"_\"+re.sub(\"historical_\",\"\",fl_hist[0]).split(\"/\")[-1][:-9]+fl_ssp[-1].split(\"/\")[-1][-9:]\n",
    "    if os.path.exists(new_fnm): \n",
    "        print(\"already processed\")\n",
    "        continue\n",
    "\n",
    "    print(\"loading... \", end = \"\")\n",
    "    ds_hist = [xr.open_dataset(fnm).tos.convert_calendar(\"standard\", align_on = \"date\") for fnm in fl_hist]\n",
    "    ds_ssp = [xr.open_dataset(fnm).tos.convert_calendar(\"standard\", align_on = \"date\") for fnm in fl_ssp]\n",
    "    \n",
    "    print(\"concatenating... \", end = \"\")\n",
    "    ds = xr.concat(ds_hist + ds_ssp, \"time\")\n",
    "\n",
    "    print(\"converting... \", end = \"\")\n",
    "    ds = convert_units_to(ds, \"degC\")\n",
    "\n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "    # SSTs are stored on a different grid to atmospheric variables - regrid\n",
    "\n",
    "    # load target grid and cut to the region we're interested in\n",
    "    tmplt = wrap_lon(xr.open_dataset(glob.glob(\"/\".join(fp.split(\"/\")[:-3])+\"/Amon/psl/*/latest/*.nc\")[0])).sel(lon = slice(xn,xx), lat = slice(yn,yx))\n",
    "\n",
    "    if \"i\" in ds.dims:   \n",
    "        # add CF attributes to allow regridding\n",
    "        ds.i.attrs['axis'] = 'X'\n",
    "        ds.j.attrs['axis'] = 'Y'\n",
    "\n",
    "    # build regridder\n",
    "    print(\"regridding... \", end = \"\")\n",
    "    rg = xe.Regridder(ds, tmplt, \"bilinear\", ignore_degenerate = True)\n",
    "    ds_rg = rg(ds).rename(\"tos\").assign_attrs(units = \"degC\")\n",
    "    ds_rg = wrap_lon(ds_rg)\n",
    "\n",
    "    print(\"saving... \", end = \"\")\n",
    "    ds_rg.to_netcdf(new_fnm)\n",
    "    print(\"\")\n",
    "clear_output(wait = False)\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f109420-4bee-4aa0-9508-ddd8843e7b19",
   "metadata": {},
   "source": [
    "# Compile other variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "80b967a7-6475-4a7e-a2ca-b35d17f99e31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n"
     ]
    }
   ],
   "source": [
    "for fp in fplist:\n",
    "\n",
    "    mdl = fp.split(\"/\")[7]+\"_\"+fp.split(\"/\")[9]\n",
    "    print(mdl, end = \": \")\n",
    "    \n",
    "    for varnm in [\"hus\", \"ta\"]:\n",
    "        print(varnm, end = \" \")\n",
    "\n",
    "        vp = \"/\".join(fp.split(\"/\")[:-3])+\"/Amon/\"+varnm+\"/*/latest/*.nc\"\n",
    "\n",
    "        fl_hist = glob.glob(re.sub(\"ScenarioMIP\", \"CMIP\", re.sub(\"ssp585\", \"historical\", vp)))\n",
    "        fl_ssp = [fnm for fnm in glob.glob(vp) if int(fnm[-16:-12]) <= 2100]\n",
    "\n",
    "        if len(fl_hist) == 0 or len(fl_ssp) == 0: continue\n",
    "\n",
    "        new_fnm = \"/home/users/clairb/potential-intensity/\"+varnm+\"/\"+rnm+\"_\"+re.sub(\"historical_\",\"\",fl_hist[0]).split(\"/\")[-1][:-9]+fl_ssp[-1].split(\"/\")[-1][-9:]\n",
    "        if os.path.exists(new_fnm): continue\n",
    "            \n",
    "        u = {\"ta\" : \"degC\", \"hus\" : \"%\"}[varnm]\n",
    "\n",
    "        # load & prep the data (cut out small region here)\n",
    "        ds_hist = [wrap_lon(xr.open_dataset(fnm))[varnm].sel(lon = slice(xn, xx), lat = slice(yn,yx)).convert_calendar(\"standard\", align_on = \"date\") for fnm in fl_hist]\n",
    "        ds_ssp = [wrap_lon(xr.open_dataset(fnm))[varnm].sel(lon = slice(xn, xx), lat = slice(yn,yx)).convert_calendar(\"standard\", align_on = \"date\") for fnm in fl_ssp]\n",
    "    \n",
    "        # compile & fix units\n",
    "        ds = xr.concat(ds_hist + ds_ssp, \"time\")\n",
    "        ds = convert_units_to(ds, u)\n",
    "        ds[\"plev\"] = convert_units_to(ds.plev, \"hPa\")\n",
    "\n",
    "        ds.to_netcdf(new_fnm)\n",
    "\n",
    "    print(\"\")\n",
    "clear_output(wait = False)\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d661a41-5114-496d-bc2a-a03eadac6c28",
   "metadata": {},
   "source": [
    "# Compute potential intensity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48f74318-af30-442d-a63e-b403f156e3e3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACCESS-CM2_r1i1p1f1_gn_185001-210012.nc\n",
      "  already processed\n",
      "ACCESS-ESM1-5_r1i1p1f1_gn_185001-210012.nc\n",
      "  already processed\n",
      "BCC-CSM2-MR_r1i1p1f1_gn_185001-210012.nc\n",
      "  already processed\n",
      "CAMS-CSM1-0_r1i1p1f1_gn_185001-209912.nc\n",
      "  already processed\n",
      "CAS-ESM2-0_r1i1p1f1_gn_185001-210012.nc\n",
      "  skipped - not all data available\n",
      "CESM2-WACCM_r1i1p1f1_gr_185001-210012.nc\n",
      "  skipped - not all data available\n",
      "CIESM_r1i1p1f1_gn_185001-210012.nc\n",
      "  skipped - not all data available\n",
      "CMCC-CM2-SR5_r1i1p1f1_gn_185001-210012.nc\n",
      "  already processed\n",
      "CMCC-ESM2_r1i1p1f1_gn_185001-210012.nc\n",
      "  already processed\n",
      "CNRM-ESM2-1_r1i1p1f2_gn_185001-210012.nc\n",
      "  skipped - not all data available\n",
      "CanESM5-CanOE_r1i1p2f1_gn_185001-210012.nc\n",
      "  already processed\n",
      "CanESM5_r1i1p1f1_gn_185001-210012.nc\n",
      "  already processed\n",
      "E3SM-1-1_r1i1p1f1_gr_185001-210012.nc\n",
      "  already processed\n",
      "EC-Earth3-CC_r1i1p1f1_gn_185001-210012.nc\n",
      "  skipped - not all data available\n",
      "EC-Earth3-Veg-LR_r1i1p1f1_gn_185001-210012.nc\n",
      "  skipped - not all data available\n",
      "EC-Earth3-Veg_r1i1p1f1_gn_185001-210012.nc\n",
      "  skipped - not all data available\n",
      "EC-Earth3_r1i1p1f1_gn_185001-210012.nc\n",
      "  skipped - not all data available\n",
      "FGOALS-f3-L_r1i1p1f1_gn_185001-210012.nc\n",
      "  skipped - not all data available\n",
      "GFDL-CM4_r1i1p1f1_gr_185001-210012.nc\n",
      "  skipped - not all data available\n",
      "GFDL-ESM4_r1i1p1f1_gr_185001-210012.nc\n",
      "  skipped - not all data available\n",
      "GISS-E2-1-G_r1i1p1f2_gn_185001-210012.nc\n",
      "  skipped - not all data available\n",
      "HadGEM3-GC31-LL_r1i1p1f3_gn_185001-210012.nc\n",
      "  already processed\n",
      "IITM-ESM_r1i1p1f1_gn_190001-209912.nc\n",
      "  skipped - not all data available\n",
      "INM-CM4-8_r1i1p1f1_gr1_185001-210012.nc\n",
      "  already processed\n",
      "INM-CM5-0_r1i1p1f1_gr1_185001-210012.nc\n",
      "  already processed\n",
      "IPSL-CM6A-LR_r1i1p1f1_gn_185001-210012.nc\n",
      "  skipped - not all data available\n",
      "KACE-1-0-G_r1i1p1f1_gr_185001-210012.nc\n",
      "  Data loaded:   11:38:04\n"
     ]
    }
   ],
   "source": [
    "fl = sorted(glob.glob(\"tos/*.nc\"))\n",
    "for fnm in fl:\n",
    "\n",
    "    mdl = \"_\".join(fnm.split(\"_\")[-4:])\n",
    "    if mdl in [\"FGOALS-g3_r1i1p1f1_gn_185001-210012.nc\"]: continue\n",
    "    print(mdl)\n",
    "\n",
    "    new_fnm = re.sub(\"tos\", \"pi\", fnm)\n",
    "    if os.path.exists(new_fnm): \n",
    "        print(\"  already processed\")\n",
    "        continue\n",
    "    \n",
    "    if not all([os.path.exists(re.sub(\"Omon\", \"Amon\", re.sub(\"tos\", varnm, fnm))) for varnm in [\"hus\", \"ta\", \"psl\"]]):\n",
    "        print(\"  skipped - not all data available\")\n",
    "        continue\n",
    "        \n",
    "    tos = xr.open_dataset(fnm).tos\n",
    "    hus, ta, psl = [xr.open_dataset(re.sub(\"Omon\", \"Amon\", re.sub(\"tos\", varnm, fnm)))[varnm] for varnm in [\"hus\", \"ta\", \"psl\"]]\n",
    "    \n",
    "    ds = xr.merge([tos, psl, ta, hus]).rename(plev = \"p\", ta = \"t\", hus = \"q\", psl = \"msl\", tos = \"sst\")\n",
    "    \n",
    "    print(\"  Data loaded:   \"+datetime.now().time().strftime(\"%H:%M:%S\"))\n",
    "    \n",
    "    # calculate the potential intensity (may take a v long time - up to 3hrs for 200 years)\n",
    "    vmax, pmin, ifl, t0, otl = xr.apply_ufunc(\n",
    "        pi,\n",
    "        ds['sst'], ds['msl'], ds['p'], ds['t'], ds['q'],\n",
    "        kwargs=dict(CKCD=0.9, ascent_flag=0, diss_flag=1, ptop=50, miss_handle=1),  # use defaults\n",
    "        input_core_dims=[\n",
    "            [], [], ['p', ], ['p', ], ['p', ],\n",
    "        ],\n",
    "        output_core_dims=[\n",
    "            [], [], [], [], []\n",
    "        ],\n",
    "        vectorize=True\n",
    "    )\n",
    "    \n",
    "    print(\"  PI calculated: \"+datetime.now().time().strftime(\"%H:%M:%S\"))\n",
    "    \n",
    "    # store the result in an xarray data structure\n",
    "    ds_out = xr.Dataset({\n",
    "        'vmax': vmax, \n",
    "        'pmin': pmin,\n",
    "        'ifl': ifl,\n",
    "        't0': t0,\n",
    "        'otl': otl,\n",
    "        })\n",
    "    \n",
    "    ds_out.to_netcdf(new_fnm)\n",
    "    \n",
    "    print(\"  Data saved:    \"+datetime.now().time().strftime(\"%H:%M:%S\"))\n",
    "clear_output(wait = False)\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22a76910-1f20-4a56-8e16-a6f3acd4c133",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fadf821-28a9-46e0-9f56-6a9bc998d417",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "xclim",
   "language": "python",
   "name": "xclim"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
